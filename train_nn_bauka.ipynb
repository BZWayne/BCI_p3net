{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_nn_bauka.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTk8pUBU2NPh",
        "outputId": "2d06144e-61b9-44fd-cf28-ea52607e834e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_EJVrxjYksN",
        "outputId": "640ad7fe-c65a-4204-87da-d17119cdd473"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  |     Proc size: 119.0 MB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total     16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxJgBVxIdfdE",
        "outputId": "c6b37fa1-0965-454c-adcd-8ae651034d5a"
      },
      "source": [
        "cd /content/drive/My Drive/Colab_Notebooks/BCI/p3net/bauka"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab_Notebooks/BCI/p3net/bauka\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "316U1_hZmuUW",
        "outputId": "a930ecdc-191d-4e8a-ec12-d85d51772be5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataloader_bauka.ipynb\tmodels_bauka.ipynb    train_utils_bauka.ipynb\n",
            "dataloader_bauka.py\tmodels_bauka.py       train_utils_bauka.py\n",
            "data_NUsubjects.pickle\t__pycache__\n",
            "model_bauyrzhan\t\ttrain_nn_bauka.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWBY4MLCcP9t",
        "outputId": "44bfd473-c1b3-4269-e6b8-6205d6d56e8d"
      },
      "source": [
        "!cat 'models_bauka.py'\n",
        "!cat 'dataloader_bauka.py'\n",
        "!cat 'train_utils_bauka.py'\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My\\ Drive/Colab_Notebooks/BCI/p3net/bauka')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"models_bauka.ipynb\n",
            "\n",
            "Automatically generated by Colaboratory.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1Vdb4SJpmfHOnfHnsK_DbQR8fvlvLR6hQ\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "from torch.autograd import Variable\n",
            "import torch.nn as nn\n",
            "import numpy as np\n",
            "import pdb\n",
            "\n",
            "# %%\n",
            "def compute_conv_dim(dim_size, kernel_size, padding, stride):\n",
            "    # compute the output dimensions of a convolutional layer\n",
            "    out_dim = int((dim_size - kernel_size + 2 * padding) / stride + 1)\n",
            "    return out_dim\n",
            "\n",
            "# %%\n",
            "class CNN2D(torch.nn.Module):\n",
            "\n",
            "    def __init__(self,\n",
            "                 input_size,  # (1, 16, 76),\n",
            "                 kernel_size=[3, 3],\n",
            "                 conv_channels=[1, 8],\n",
            "                 dense_size=256,\n",
            "                 dropout=0.1):\n",
            "\n",
            "        super(CNN2D, self).__init__()\n",
            "        self.cconv = []\n",
            "        self.MaxPool = nn.MaxPool2d((1, 2), (1, 2))\n",
            "        self.ReLU = nn.ReLU()\n",
            "        self.Dropout = nn.Dropout(dropout)\n",
            "        self.batchnorm = []\n",
            "\n",
            "        for jj in conv_channels:\n",
            "            self.batchnorm.append(nn.BatchNorm2d(jj).cuda())\n",
            "        ii = 0\n",
            "        # define CONV layer architecture:\n",
            "        for in_channels, out_channels in zip(conv_channels, conv_channels[1:]):\n",
            "            conv_i = torch.nn.Conv2d(in_channels=in_channels,\n",
            "                                     out_channels=out_channels,\n",
            "                                     kernel_size=kernel_size[ii],\n",
            "                                     padding=kernel_size[ii]//2)\n",
            "            self.cconv.append(conv_i)\n",
            "            self.add_module('CNN_K{}_O{}'.format(\n",
            "                kernel_size[ii], out_channels), conv_i)\n",
            "            ii += 1\n",
            "\n",
            "        self.flat_fts = self.get_output_dim(input_size, self.cconv)\n",
            "        self.fc1 = torch.nn.Linear(self.flat_fts, dense_size)\n",
            "        self.fc2 = torch.nn.Linear(dense_size, 2)\n",
            "\n",
            "    def get_output_dim(self, in_size, cconv):\n",
            "        with torch.no_grad():\n",
            "            input = Variable(torch.ones(1, *in_size))\n",
            "            for conv_i in self.cconv:\n",
            "                input = conv_i(input)\n",
            "                input = self.MaxPool(input)\n",
            "                print('>>> Conv Output >>>', input.shape)\n",
            "                flatout = int(np.prod(input.size()[1:]))\n",
            "\n",
            "            print(\"Flattened output ::\", flatout)\n",
            "        return flatout\n",
            "\n",
            "    def forward(self, input):\n",
            "        for jj, conv_i in enumerate(self.cconv):\n",
            "            input = conv_i(input)\n",
            "            input = self.batchnorm[jj+1](input)\n",
            "            input = self.ReLU(input)\n",
            "            input = self.MaxPool(input)\n",
            "\n",
            "        # flatten the CNN output\n",
            "        out = input.view(-1, self.flat_fts)\n",
            "        out = self.ReLU(self.fc1(out))\n",
            "        out = self.Dropout(out)\n",
            "        out = self.fc2(out)\n",
            "        return out\n",
            "\n",
            "# %% ## _LSTM Model\n",
            "\n",
            "\n",
            "class LSTM_Model(torch.nn.Module):\n",
            "\n",
            "    def __init__(self, input_size, hidden_size=128, num_layers=1, dropout=0.1):\n",
            "        super(LSTM_Model, self).__init__()\n",
            "\n",
            "        self.hidden_size = hidden_size\n",
            "        self.num_layers = num_layers\n",
            "\n",
            "        # LSTM input dimension is: (batch_size, time_steps, num_features)\n",
            "        # LSTM output dimension is: (batch_size, time_steps, hidden_size)\n",
            "        self.lstm = torch.nn.LSTM(input_size=input_size,\n",
            "                                  hidden_size=hidden_size,\n",
            "                                  num_layers=num_layers,\n",
            "                                  batch_first=True,\n",
            "                                  dropout=dropout)\n",
            "\n",
            "        self.dropout = torch.nn.Dropout(dropout)\n",
            "        self.fc = torch.nn.Linear(hidden_size, 2)\n",
            "\n",
            "    def forward(self, x, hidden):\n",
            "        self.lstm.flatten_parameters()  # For deep copy\n",
            "        # output dimension is: (batch_size, time_steps, hidden_size)\n",
            "        # Take only last output of LSTM (many-to-one RNN)\n",
            "        x = self.lstm(x, hidden)[0][:, -1, :]\n",
            "        # hidden_size contains all outputs [y] values (each LSTM cells produces one output)\n",
            "        x = x.view(x.shape[0], -1)  # Flatten to (batch_size, hidden_size)\n",
            "        x = self.dropout(x)\n",
            "        x = self.fc(x)\n",
            "        return x\n",
            "\n",
            "    def init_hidden(self, batch_size):\n",
            "        '''\n",
            "        Initializing the hidden layer.\n",
            "        Call every mini-batch, since nn.LSTM does not reset it itself.\n",
            "        '''\n",
            "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
            "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
            "\n",
            "        if torch.cuda.is_available():\n",
            "            return (Variable(h_0.cuda()), Variable(c_0.cuda()))\n",
            "        else:\n",
            "            return (Variable(h_0), Variable(c_0))\n",
            "\n",
            "# %% CNN LSTM model\n",
            "\n",
            "\n",
            "class CNN2DEncoder(torch.nn.Module):\n",
            "\n",
            "    def __init__(self,\n",
            "                 kernel_size=[3, 3, 3, 3],\n",
            "                 conv_channels=[1, 8, 16, 32],\n",
            "                 dense_size=256,\n",
            "                 dropout=0.1):\n",
            "        super(CNN2DEncoder, self).__init__()\n",
            "        self.cconv = []\n",
            "        self.MaxPool = nn.MaxPool2d((1, 2), (1, 2))\n",
            "        self.ReLU = nn.ReLU()\n",
            "        self.Dropout = nn.Dropout(dropout)\n",
            "        self.batchnorm = []\n",
            "\n",
            "        for jj in conv_channels:\n",
            "            self.batchnorm.append(nn.BatchNorm2d(jj).cuda())\n",
            "        ii = 0\n",
            "        for in_channels, out_channels in zip(conv_channels, conv_channels[1:]):\n",
            "            conv_i = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
            "                                     kernel_size=kernel_size[ii], padding=kernel_size[ii]//2)\n",
            "            self.cconv.append(conv_i)\n",
            "            self.add_module('CNN_K{}_O{}'.format(\n",
            "                kernel_size[ii], out_channels), conv_i)\n",
            "            ii += 1\n",
            "\n",
            "    def forward(self, input):\n",
            "        for jj, conv_i in enumerate(self.cconv):\n",
            "            input = conv_i(input)\n",
            "            input = self.batchnorm[jj+1](input)\n",
            "            input = self.ReLU(input)\n",
            "            input = self.MaxPool(input)\n",
            "        return input\n",
            "\n",
            "# %%\n",
            "\n",
            "\n",
            "class CNNLSTM(torch.nn.Module):\n",
            "\n",
            "    def __init__(self, input_size, cnn, hidden_size=128, num_layers=1,\n",
            "                 batch_size=16, dropout=0.1):\n",
            "        super(CNNLSTM, self).__init__()\n",
            "\n",
            "        self.cnn = cnn\n",
            "        self.hidden_size = hidden_size\n",
            "        self.num_layers = num_layers\n",
            "\n",
            "        # LSTM input dimension is: (batch_size, time_steps, num_features)\n",
            "        # LSTM output dimension is: (batch_size, time_steps, hidden_size)\n",
            "        self.lstm = torch.nn.LSTM(input_size=input_size,\n",
            "                                  hidden_size=hidden_size,\n",
            "                                  num_layers=num_layers,\n",
            "                                  batch_first=True,\n",
            "                                  dropout=dropout)\n",
            "\n",
            "        self.dropout = torch.nn.Dropout(dropout)\n",
            "        self.fc = torch.nn.Linear(hidden_size, 2)\n",
            "\n",
            "    def forward(self, input, hidden):\n",
            "\n",
            "        encoder = self.cnn(input)\n",
            "        # pdb.set_trace()\n",
            "        self.lstm.flatten_parameters()\n",
            "\n",
            "        batch_size, timesteps, H, W = encoder.size()\n",
            "        r_in = encoder.view(batch_size, timesteps, -1)\n",
            "\n",
            "        # output dimension is: (batch_size, time_steps, hidden_size)\n",
            "        # Take only last output of LSTM (many-to-one RNN)\n",
            "        x = self.lstm(r_in, hidden)[0][:, -1, :]\n",
            "\n",
            "        # hidden_size contains all outputs [y] values (each LSTM cells produces one output)\n",
            "        x = x.view(x.shape[0], -1)  # Flatten to (batch_size, hidden_size)\n",
            "        x = self.dropout(x)\n",
            "        x = self.fc(x)\n",
            "        return x\n",
            "\n",
            "    def init_hidden(self, batch):\n",
            "\n",
            "        h_0 = torch.zeros(self.num_layers, batch, self.hidden_size)\n",
            "        c_0 = torch.zeros(self.num_layers, batch, self.hidden_size)\n",
            "\n",
            "        if torch.cuda.is_available():\n",
            "            return (Variable(h_0.cuda()), Variable(c_0.cuda()))\n",
            "        else:\n",
            "            return (Variable(h_0), Variable(c_0))\n",
            "\n",
            "# %%\n",
            "\n",
            "\n",
            "class CNN(nn.Module):\n",
            "    def __init__(self, input_size):\n",
            "        super(CNN, self).__init__()\n",
            "        # convolutional layer (sees 1x16x76 image tensor)\n",
            "        self.features = nn.Sequential(\n",
            "            nn.Conv2d(1, 16, 3, padding=1),\n",
            "            nn.BatchNorm2d(16),\n",
            "            nn.ReLU(),\n",
            "            nn.MaxPool2d(2, 2),\n",
            "\n",
            "            nn.Conv2d(16, 32, 3, padding=1),\n",
            "            nn.BatchNorm2d(32),\n",
            "            nn.ReLU(),\n",
            "            nn.MaxPool2d(2, 2),\n",
            "\n",
            "            nn.Conv2d(32, 64, 3, padding=1),\n",
            "            nn.BatchNorm2d(64),\n",
            "            nn.ReLU(),\n",
            "            nn.MaxPool2d(2, 2))\n",
            "\n",
            "        self.flat_fts = self.get_out_dim(input_size, self.features)\n",
            "\n",
            "        self.classifier = nn.Sequential(\n",
            "            nn.Linear(self.flat_fts, 200),\n",
            "            nn.Dropout(0.25),\n",
            "            nn.Linear(200, 2))\n",
            "\n",
            "    def get_out_dim(self, in_size, fts):\n",
            "        with torch.no_grad():\n",
            "            f = fts(Variable(torch.ones(1, *in_size)))\n",
            "            out = int(np.prod(f.size()[1:]))\n",
            "            return out\n",
            "\n",
            "    def forward(self, x):\n",
            "        # add sequence of convolutional and max pooling layers\n",
            "        fts = self.features(x)\n",
            "        flat_fts = fts.view(-1, self.flat_fts)\n",
            "        return self.classifier(flat_fts)\n",
            "\n",
            "# %%\n",
            "\n",
            "\n",
            "class EEGNet(nn.Module):\n",
            "    def __init__(self,\n",
            "                 time_samples,\n",
            "                 channels):\n",
            "\n",
            "        super(EEGNet, self).__init__()\n",
            "\n",
            "        self.T = time_samples\n",
            "        self.chans = channels\n",
            "        self.in_size = (1, time_samples, channels)\n",
            "\n",
            "        self.layer1 = nn.Sequential(\n",
            "            # Layer 1\n",
            "            nn.Conv2d(1, 16, (1, self.chans), padding=0),\n",
            "            nn.BatchNorm2d(16, False),\n",
            "            nn.ELU(),\n",
            "            nn.Dropout(0.25))\n",
            "\n",
            "        self.layer2and3 = nn.Sequential(\n",
            "            # Layer 2\n",
            "            nn.ZeroPad2d((16, 17, 0, 1)),\n",
            "            nn.Conv2d(1, 4, (2, 32)),\n",
            "            nn.BatchNorm2d(4, False),\n",
            "            nn.ELU(),\n",
            "            nn.Dropout(0.25),\n",
            "            nn.MaxPool2d(2, 4),\n",
            "\n",
            "            # Layer 3\n",
            "            nn.ZeroPad2d((2, 1, 4, 3)),\n",
            "            nn.Conv2d(4, 4, (8, 4)),\n",
            "            nn.BatchNorm2d(4, False),\n",
            "            nn.Dropout(0.25),\n",
            "            nn.MaxPool2d((2, 4)))\n",
            "\n",
            "        self.flat_fts = self.get_out_dim(self.in_size)\n",
            "        self.fc1 = nn.Linear(self.flat_fts, 2)\n",
            "\n",
            "    def get_out_dim(self, in_size):\n",
            "        with torch.no_grad():\n",
            "            # create a tensor\n",
            "            x = Variable(torch.ones(1, *self.in_size))\n",
            "            x = self.layer1(x)\n",
            "            x = x.permute(0, 3, 1, 2)\n",
            "            x = self.layer2and3(x)\n",
            "            x = int(np.prod(x.size()[1:]))\n",
            "            return x\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.layer1(x)\n",
            "        x = x.permute(0, 3, 1, 2)\n",
            "\n",
            "        x = self.layer2and3(x)\n",
            "        x = x.view(-1, self.flat_fts)\n",
            "        x = self.fc1(x)\n",
            "        return x# -*- coding: utf-8 -*-\n",
            "\"\"\"dataloader_bauka.ipynb\n",
            "\n",
            "Automatically generated by Colaboratory.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1aUsNgIujU0u5Fe1OFtXo1tZQbdfpKUpy\n",
            "\"\"\"\n",
            "\n",
            "import pickle\n",
            "import numpy as np\n",
            "import torch\n",
            "from sklearn.model_selection import train_test_split\n",
            "from torch.utils.data import TensorDataset, DataLoader\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.base import TransformerMixin\n",
            "\n",
            "class getTorch(object):\n",
            "    def __init__(self):\n",
            "        pass\n",
            "\n",
            "    @staticmethod\n",
            "    def get_data(data, batch_size, lstm, image, raw):\n",
            "\n",
            "        # Input data is a dictionary\n",
            "        x_train, y_train = data['xtrain'], data['ytrain']\n",
            "        x_valid, y_valid = data['xvalid'], data['yvalid']\n",
            "        x_test,  y_test = data['xtest'],  data['ytest']\n",
            "\n",
            "        if lstm:  # re-arranges the data to use with LSTM\n",
            "            x_train = x_train.permute(0, 2, 1)\n",
            "            x_valid = x_valid.permute(0, 2, 1)\n",
            "            x_test = x_test.permute(0, 2, 1)\n",
            "\n",
            "        if image:  # this option will reshape the input as a gray scale image\n",
            "            x_train = torch.unsqueeze(x_train, dim=1)\n",
            "            x_valid = torch.unsqueeze(x_valid, dim=1)\n",
            "            x_test = torch.unsqueeze(x_test, dim=1)\n",
            "\n",
            "        print('Input data shape', x_train.shape)\n",
            "        ##############################################\n",
            "        # TensorDataset\n",
            "        train_dat = TensorDataset(x_train, y_train)\n",
            "        val_dat = TensorDataset(x_valid, y_valid)\n",
            "\n",
            "        ##############################################\n",
            "        train_loader = DataLoader(\n",
            "            train_dat, batch_size=batch_size, shuffle=True, drop_last=False)\n",
            "        val_loader = DataLoader(\n",
            "            val_dat,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
            "\n",
            "        if raw:  # get the raw inputs (no TensorDataset nor DataLoader used)\n",
            "            out = dict(train_input=x_train,\n",
            "                       x_valid=x_valid,\n",
            "                       train_target=y_train,\n",
            "                       y_valid=y_valid,\n",
            "                       test_data={'x_test': x_test, 'y_test': y_test})\n",
            "\n",
            "        else:  # return data loaders\n",
            "            out = dict(dset_loaders={'train': train_loader, 'val': val_loader},\n",
            "                       dset_sizes={'train': len(x_train), 'val': len(x_valid)},\n",
            "                       test_data={'x_test': x_test, 'y_test': y_test})\n",
            "        return out\n",
            "\n",
            "    @staticmethod\n",
            "    def get_dataEEGnet(data, batch_size, lstm, image):\n",
            "\n",
            "        # Input data is a dictionary\n",
            "        x_train, y_train = data['xtrain'], data['ytrain']\n",
            "        x_valid, y_valid = data['xvalid'], data['yvalid']\n",
            "        x_test,  y_test = data['xtest'],  data['ytest']\n",
            "\n",
            "        if lstm:  # re-arranges the data to use with LSTM\n",
            "            x_train = x_train.permute(0, 2, 1)\n",
            "            x_valid = x_valid.permute(0, 2, 1)\n",
            "            x_test = x_test.permute(0, 2, 1)\n",
            "\n",
            "        if image:  # this option will reshape the input as a gray scale image\n",
            "            x_train = torch.unsqueeze(x_train, dim=-1)\n",
            "            x_valid = torch.unsqueeze(x_valid, dim=-1)\n",
            "            x_test = torch.unsqueeze(x_test, dim=-1)\n",
            "\n",
            "        print('Input data shape', x_train.shape)\n",
            "        # TensorDataset\n",
            "        train_dat = TensorDataset(x_train, y_train)\n",
            "        val_dat = TensorDataset(x_valid, y_valid)\n",
            "        train_loader = DataLoader(\n",
            "            train_dat, batch_size=batch_size, shuffle=True)\n",
            "        val_loader = DataLoader(\n",
            "            val_dat,   batch_size=batch_size, shuffle=False)\n",
            "\n",
            "        return dict(dset_loaders={'train': train_loader, 'val': val_loader},\n",
            "                    dset_sizes={'train': len(x_train), 'val': len(x_valid)},\n",
            "                    test_data={'x_test': x_test, 'y_test': y_test})\n",
            "\n",
            "class SKStandardScaler(TransformerMixin):\n",
            "    def __init__(self, **kwargs):\n",
            "        self._scaler = StandardScaler(copy=True, **kwargs)\n",
            "        self._orig_shape = None\n",
            "\n",
            "    def fit(self, X, **kwargs):\n",
            "        X = np.array(X)\n",
            "        if len(X.shape) > 1:\n",
            "            self._orig_shape = X.shape[1:]\n",
            "        X = self._flatten(X)\n",
            "        self._scaler.fit(X, **kwargs)\n",
            "        return self\n",
            "\n",
            "    def transform(self, X, **kwargs):\n",
            "        X = np.array(X)\n",
            "        X = self._flatten(X)\n",
            "        X = self._scaler.transform(X, **kwargs)\n",
            "        X = self._reshape(X)\n",
            "        return X\n",
            "\n",
            "    def _flatten(self, X):\n",
            "        if len(X.shape) > 2:\n",
            "            n_dims = np.prod(self._orig_shape)\n",
            "            X = X.reshape(-1, n_dims)\n",
            "        return X\n",
            "\n",
            "    def _reshape(self, X):\n",
            "        if len(X.shape) >= 2:\n",
            "            X = X.reshape(-1, *self._orig_shape)\n",
            "        return X\n",
            "\n",
            "class EEGDataLoader(object):\n",
            "    def __init__(self, filename, datapath=\"\"):\n",
            "        self.filename = filename\n",
            "        self.datapath = datapath\n",
            "\n",
            "    def load_pooled(self, subjectIndex):\n",
            "\n",
            "        with open(self.filename, 'rb') as handle:\n",
            "            b = pickle.load(handle)\n",
            "\n",
            "        # %% extract positive and negative classes\n",
            "        pos, neg = [], []\n",
            "        # filetype is used for correct indexing in NU and MOABB datasets\n",
            "        # default MNE keys are set to:\n",
            "        try:\n",
            "            if b[0]['pos']:\n",
            "                target, nontarget = 'pos', 'neg'\n",
            "            print('Working with NU data')\n",
            "        except Exception:\n",
            "            target, nontarget = 'Target', 'NonTarget'\n",
            "            print('Working with MOABB data')\n",
            "\n",
            "        for ii in subjectIndex:\n",
            "            try:\n",
            "                pos.append(b[ii][target].get_data())\n",
            "                neg.append(b[ii][nontarget].get_data())\n",
            "            except Exception as err:\n",
            "                print(err)\n",
            "\n",
            "        # %% prepare the pooled data / concatenate data from all subjects\n",
            "        s1pos = pos[-1]  # get the data from the last subject in the list\n",
            "        s1neg = neg[-1]\n",
            "        for jj in range(len(pos)-1):  # all subject but the last one\n",
            "            p1, n1 = pos[jj], neg[jj]\n",
            "            s1pos = np.concatenate([s1pos, p1])\n",
            "            s1neg = np.concatenate([s1neg, n1])\n",
            "\n",
            "        # %% get the labels and construct data array from all subjects\n",
            "        X, Y = [], []\n",
            "        X = np.concatenate([s1pos.astype('float32'), s1neg.astype('float32')])\n",
            "        Y = np.concatenate([np.ones(s1pos.shape[0]).astype(\n",
            "            'float32'), np.zeros(s1neg.shape[0]).astype('float32')])\n",
            "\n",
            "        # %% normalization\n",
            "        scaler = SKStandardScaler()\n",
            "        X = scaler.fit_transform(X)\n",
            "\n",
            "        x_rest, x_test, y_rest, y_test =\\\n",
            "            train_test_split(X, Y, test_size=0.20, random_state=42, stratify=Y)\n",
            "\n",
            "        x_train, x_valid, y_train, y_valid =\\\n",
            "            train_test_split(x_rest, y_rest, test_size=0.25,\n",
            "                             random_state=42, stratify=y_rest)\n",
            "\n",
            "# %%     UPSAMPLING after split / x_test is left out without upsampling\n",
            "        upTrain = len(y_train[y_train == 0])//len(y_train[y_train == 1])\n",
            "        ptrain = x_train[y_train == 1, :, :]\n",
            "        pvalid = x_valid[y_valid == 1, :, :]\n",
            "\n",
            "        for j in range(upTrain-1):  # create multiple copies of target ERPs\n",
            "            ptrain = np.concatenate([ptrain, x_train[y_train == 1, :, :]])\n",
            "            pvalid = np.concatenate([pvalid, x_valid[y_valid == 1, :, :]])\n",
            "\n",
            "        # upsampled xtrain\n",
            "        x_train = np.concatenate([x_train, ptrain])\n",
            "        y_train = np.concatenate([y_train, np.ones(ptrain.shape[0])])\n",
            "\n",
            "        # upsampled xvalid\n",
            "        x_valid = np.concatenate([x_valid, pvalid])\n",
            "        y_valid = np.concatenate([y_valid, np.ones(pvalid.shape[0])])\n",
            "\n",
            "        # Convert to Pytorch tensors\n",
            "        X_train, X_valid, X_test = map(\n",
            "            torch.FloatTensor, (x_train, x_valid, x_test))\n",
            "        y_train, y_valid, y_test = map(\n",
            "            torch.LongTensor,  (y_train, y_valid, y_test))\n",
            "\n",
            "        return dict(xtrain=X_train, xvalid=X_valid, xtest=X_test,\n",
            "                    ytrain=y_train, yvalid=y_valid, ytest=y_test)\n",
            "\n",
            "    # %% returns subject specific data dictionary with xtrain, xvalid, xtest\n",
            "    def subject_specific(self, subjectIndex):\n",
            "        with open(self.filename, 'rb') as handle:\n",
            "            b = pickle.load(handle)\n",
            "        # extract positive and negative classes\n",
            "        pos, neg = [], []\n",
            "        datt = []\n",
            "       # filetype is used for correct indexing in NU and MOABB datasets\n",
            "        # default MNE keys are set to:\n",
            "        try:\n",
            "            if b[0]['pos']:\n",
            "                target, nontarget = 'pos', 'neg'\n",
            "        except Exception:\n",
            "            target, nontarget = 'Target', 'NonTarget'\n",
            "\n",
            "        if len(subjectIndex) > 1:\n",
            "            try:  # so that subject index could exceed the number of available datasets\n",
            "                for jj in subjectIndex:\n",
            "                    print('Loading subjects:', jj)\n",
            "                    dat = b[jj]\n",
            "                    pos.append(dat[target].get_data())\n",
            "                    neg.append(dat[nontarget].get_data())\n",
            "            except Exception as err:\n",
            "                print(err)\n",
            "        else:\n",
            "            print('Loading subject:', subjectIndex[0]+1)\n",
            "            dat = b[subjectIndex[0]]\n",
            "            pos.append(dat[target].get_data())\n",
            "            neg.append(dat[nontarget].get_data())\n",
            "\n",
            "        # subject specific upsampling\n",
            "        for ii in range(len(pos)):\n",
            "            X, Y = [], []\n",
            "            X = np.concatenate(\n",
            "                [pos[ii].astype('float32'), neg[ii].astype('float32')])\n",
            "            Y = np.concatenate([np.ones(pos[ii].shape[0]).astype(\n",
            "                'float32'), np.zeros(neg[ii].shape[0]).astype('float32')])\n",
            "\n",
            "            # % normalization\n",
            "            scaler = SKStandardScaler()\n",
            "            X = scaler.fit_transform(X)\n",
            "\n",
            "            x_rest, x_test, y_rest, y_test =\\\n",
            "                train_test_split(X, Y, test_size=0.2,\n",
            "                                 random_state=42, stratify=Y)\n",
            "\n",
            "            x_train, x_valid, y_train, y_valid =\\\n",
            "                train_test_split(x_rest, y_rest, test_size=0.2,\n",
            "                                 random_state=42, stratify=y_rest)\n",
            "\n",
            "            # % UPSAMPLING Begin\n",
            "            upTrain = len(y_train[y_train == 0])//len(y_train[y_train == 1])\n",
            "            ptrain = x_train[y_train == 1, :, :]\n",
            "            pvalid = x_valid[y_valid == 1, :, :]\n",
            "\n",
            "            for j in range(upTrain-1):\n",
            "                ptrain = np.concatenate([ptrain, x_train[y_train == 1, :, :]])\n",
            "                pvalid = np.concatenate([pvalid, x_valid[y_valid == 1, :, :]])\n",
            "\n",
            "            # upsampled xtrain\n",
            "            x_train = np.concatenate([x_train, ptrain])\n",
            "            y_train = np.concatenate([y_train, np.ones(ptrain.shape[0])])\n",
            "\n",
            "            # upsampled xvalid\n",
            "            x_valid = np.concatenate([x_valid, pvalid])\n",
            "            y_valid = np.concatenate([y_valid, np.ones(pvalid.shape[0])])\n",
            "\n",
            "           # Convert to Pytorch tensors\n",
            "            X_train, X_valid, X_test = map(\n",
            "                torch.FloatTensor, (x_train, x_valid, x_test))\n",
            "            y_train, y_valid, y_test = map(\n",
            "                torch.LongTensor,  (y_train, y_valid, y_test))\n",
            "\n",
            "            datt.append(dict(xtrain=X_train, xvalid=X_valid, xtest=X_test,\n",
            "                             ytrain=y_train, yvalid=y_valid, ytest=y_test))\n",
            "        return datt# -*- coding: utf-8 -*-\n",
            "\"\"\"train_utils_bauka.ipynb\n",
            "\n",
            "Automatically generated by Colaboratory.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1ViaMdTexEzndOAmRIzPEu7Umk4PKJVF6\n",
            "\"\"\"\n",
            "\n",
            "import numpy as np\n",
            "import time\n",
            "import copy\n",
            "import torch\n",
            "from torch.utils.data import TensorDataset, DataLoader\n",
            "from models_bauka import LSTM_Model, CNNLSTM\n",
            "import pdb\n",
            "\n",
            "def train_model(model, dset_loaders, dset_sizes, criterion, optimizer, dev,\n",
            "                lr_scheduler=None, num_epochs=50, verbose=2):\n",
            "\n",
            "    start_time = time.time()\n",
            "    best_model, best_acc = model, 0.0\n",
            "\n",
            "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
            "    train_labels, val_labels = [], []\n",
            "\n",
            "    for epoch in range(num_epochs):\n",
            "        if verbose > 1:\n",
            "            print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
            "        ypred_labels, ytrue_labels = [], []\n",
            "\n",
            "        # there are two phases [Train and Validation]\n",
            "        for phase in ['train', 'val']:\n",
            "            if phase == 'train':\n",
            "                if lr_scheduler:\n",
            "                    optimizer = lr_scheduler(optimizer, epoch)\n",
            "                model.train(True)  # Set model to training mode\n",
            "            else:\n",
            "                model.train(False)  # Set model to evaluate mode\n",
            "            running_loss, running_corrects = 0.0, 0.0\n",
            "            # Iterate over mini-batches\n",
            "            batch = 0\n",
            "            for data in dset_loaders[phase]:\n",
            "                input, label = data\n",
            "                inputs = input.to(dev)\n",
            "                labels = label.to(dev)\n",
            "                model = model.cuda()\n",
            "                optimizer.zero_grad()\n",
            "\n",
            "                if isinstance(model, LSTM_Model) or isinstance(model, CNNLSTM):\n",
            "                    hidden = model.init_hidden(\n",
            "                        len(inputs))  # Zero the hidden state\n",
            "                    preds = model(inputs, hidden)  # Forward pass\n",
            "                else:\n",
            "                    # pdb.set_trace()\n",
            "                    preds = model(inputs)  # Forward pass\n",
            "                # Calculate the loss\n",
            "                loss = criterion(preds, labels)\n",
            "                # Backpropagate & weight update\n",
            "                if phase == 'train':\n",
            "                    loss.backward()\n",
            "                    optimizer.step()\n",
            "                # store batch performance\n",
            "                running_loss += loss.item()\n",
            "                preds_classes = preds.data.max(1)[1]\n",
            "                running_corrects += torch.sum(preds_classes == labels.data)\n",
            "\n",
            "                ytrue_labels.append(labels.data.cpu().detach().numpy())\n",
            "                ypred_labels.append(preds_classes.cpu().detach().numpy())\n",
            "\n",
            "                batch += 1\n",
            "\n",
            "            epoch_loss = running_loss / dset_sizes[phase]\n",
            "            epoch_acc = running_corrects.cpu().numpy()/dset_sizes[phase]\n",
            "\n",
            "            if phase == 'train':\n",
            "                train_losses.append(epoch_loss)\n",
            "                train_accs.append(epoch_acc)\n",
            "                train_labels.append(\n",
            "                    dict(ypred=ypred_labels, ytrue=ytrue_labels))\n",
            "\n",
            "            else:  # val\n",
            "                val_losses.append(epoch_loss)\n",
            "                val_accs.append(epoch_acc)\n",
            "                val_labels.append(dict(ypred=ypred_labels, ytrue=ytrue_labels))\n",
            "\n",
            "            if verbose > 1:\n",
            "                print('{} loss: {:.4f}, acc: {:.4f}'.format(\n",
            "                    phase, epoch_loss, epoch_acc))\n",
            "            # Deep copy the best model using early stopping\n",
            "            if phase == 'val' and epoch_acc > best_acc:\n",
            "                best_acc = epoch_acc\n",
            "                best_epoch = epoch\n",
            "                best_model = copy.deepcopy(model)\n",
            "\n",
            "    time_elapsed = time.time() - start_time\n",
            "\n",
            "    # ytrue and ypred from the best model during the training\n",
            "    def best_epoch_labels(train_labels, best_epoch):\n",
            "        for jj in range(len(train_labels[best_epoch]['ypred'])-1):\n",
            "            if jj == 0:\n",
            "                ypred = train_labels[best_epoch]['ypred'][jj]\n",
            "                ytrue = train_labels[best_epoch]['ytrue'][jj]\n",
            "            ypred = np.concatenate(\n",
            "                [ypred, train_labels[best_epoch]['ypred'][jj+1]])\n",
            "            ytrue = np.concatenate(\n",
            "                [ytrue, train_labels[best_epoch]['ytrue'][jj+1]])\n",
            "        return ypred, ytrue\n",
            "\n",
            "    ytrain_best = best_epoch_labels(train_labels, best_epoch)\n",
            "    yval_best = best_epoch_labels(val_labels, best_epoch)\n",
            "\n",
            "    info = dict(ytrain=ytrain_best, yval=yval_best,\n",
            "                best_epoch=best_epoch, best_acc=best_acc)\n",
            "\n",
            "    if verbose > 0:\n",
            "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
            "            time_elapsed // 60, time_elapsed % 60))\n",
            "        print('Best val Acc: {:4f}'.format(best_acc))\n",
            "        print('Best Epoch :', best_epoch+1)\n",
            "    return best_model, train_losses, val_losses, train_accs, val_accs, info"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uki0CI6bnRf"
      },
      "source": [
        "import torch \n",
        "import pandas as pd \n",
        "import pickle\n",
        "\n",
        "from dataloader_bauka import getTorch, EEGDataLoader\n",
        "from train_utils_bauka import train_model\n",
        "from models_bauka import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ijy22872Npj",
        "outputId": "a7e2e478-6327-4603-df3f-5d185a611523"
      },
      "source": [
        "dev = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA available!\")\n",
        "    dev = torch.device(\"cuda\")\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "\n",
        "get_data = getTorch.get_data \n",
        "\n",
        "# Load ERP data\n",
        "dname = dict(nu   = 'data_NUsubjects.pickle')\n",
        "\n",
        "#%% Hyperparameter settings \n",
        "\n",
        "num_epochs = 30\n",
        "batch_size = 8\n",
        "verbose = 2\n",
        "learning_rate = 1e-2\n",
        "weight_decay = 1e-4                   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA available!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW_D4ByPwujj",
        "outputId": "d5b73443-3b10-44af-c91c-cf7dc48e97a5"
      },
      "source": [
        "print(dname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'nu': 'data_NUsubjects.pickle'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNElIAKl_3Y0",
        "outputId": "eaf2913d-8ec6-433e-c357-df0cb45ecc6d"
      },
      "source": [
        "!pip3 install mne"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (0.22.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6DgDeITlbDM",
        "outputId": "001aea5a-a90c-47ee-f415-c5179824f3ae"
      },
      "source": [
        "for itemname, filename in dname.items():\n",
        "\n",
        "    print('working with', filename)\n",
        "    iname = itemname + '__'   \n",
        "    \n",
        "    if filename =='data_NUsubjects.pickle':\n",
        "        d  =  {'conv_channels': [1, 256, 128, 64, 32, 16, 8], # convolutional layers and number of filters in each layer \n",
        "               'kernel_size':   [3, 3, 3, 3, 3, 3], # kernel size \n",
        "               'num_layers':    1, # number of LSTM layers\n",
        "               'hidden_size':   128} # number of neurons in each LSTM layer \n",
        "  \n",
        "    \n",
        "    # EEG data loader \n",
        "    dd = EEGDataLoader(filename)\n",
        "    \n",
        "    # load subject specific data with subject data indicies  \n",
        "    s = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]   \n",
        "    data = dd.subject_specific(s)\n",
        "    print(data[0].keys())\n",
        "        \n",
        "    #% identify input size (channel x timepoints)\n",
        "    timelength = data[0]['xtrain'].shape[2]\n",
        "    chans      = data[0]['xtrain'].shape[1]\n",
        " \n",
        "    datum = {}\n",
        "    # get torch data loaders for each subject data \n",
        "    for ii in range(len(data)):\n",
        "      datum[ii] = get_data(data[ii], batch_size, image = True, lstm = False, raw = False)\n",
        "   \n",
        "    # used for storing results later \n",
        "    results, models = {}, {}  \n",
        "    table = pd.DataFrame(columns = ['Train_Loss', 'Val_Loss', 'Train_Acc', \n",
        "                                        'Val_Acc', 'Test_Acc', 'Epoch'])\n",
        "    \n",
        "    # for each subject in a given data type perform model selection\n",
        "    for subjectIndex in datum:    \n",
        "        dset_loaders = datum[subjectIndex]['dset_loaders']\n",
        "        dset_sizes   = datum[subjectIndex]['dset_sizes']      \n",
        "        \n",
        "        input_size = datum[subjectIndex]['test_data']['x_test'].shape\n",
        "\n",
        "        # define the encoder model                  \n",
        "        encoder = CNN2DEncoder(kernel_size = d['kernel_size'], \n",
        "                               conv_channels = d['conv_channels'])      \n",
        "        # we need to pass a sample data to get the output dimensionality of CNN \n",
        "        with torch.no_grad():\n",
        "            x = torch.randn(input_size)\n",
        "            outdim = encoder(x)\n",
        "            batch_size, chans, H, W = outdim.size()\n",
        "            \n",
        "        # define the encoder-decoder model \n",
        "        model = CNNLSTM(input_size = H*W, \n",
        "                        cnn = encoder,\n",
        "                        hidden_size = d['hidden_size'], \n",
        "                        num_layers  = d['num_layers'], \n",
        "                        batch_size  = 8,\n",
        "                        dropout     = 0.2)      \n",
        "        \n",
        "        # define the optimizer and the loss function \n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        \n",
        "        # move to GPU/CPU\n",
        "        model.to(dev)  \n",
        "        criterion.to(dev)           \n",
        "        \n",
        "        # the training loop\n",
        "        best_model, train_losses, val_losses, train_accs, val_accs, info = train_model(model, dset_loaders, \n",
        "                                                                                       dset_sizes, criterion, \n",
        "                                                                                       optimizer, dev, \n",
        "                                                                                       lr_scheduler = None, \n",
        "                                                                                       num_epochs = num_epochs, \n",
        "                                                                                       verbose = verbose)    \n",
        "         \n",
        "        # here train_model returns the best_model which is saved for a later use below        \n",
        "        # we could immediately evaluate the best model on the test as\n",
        "        x_test = datum[subjectIndex]['test_data']['x_test'] \n",
        "        y_test = datum[subjectIndex]['test_data']['y_test'] \n",
        "        \n",
        "        h=best_model.init_hidden(x_test.shape[0])\n",
        "        preds = best_model(x_test.to(dev),h)    \n",
        "        preds_class = preds.data.max(1)[1]\n",
        "        \n",
        "        # accuracy metric \n",
        "        corrects = torch.sum(preds_class == y_test.data.to(dev))     \n",
        "        test_acc = corrects.cpu().numpy()/x_test.shape[0]    \n",
        "        print(\"Test Accuracy :\", test_acc)    \n",
        "        \n",
        "        # save results       \n",
        "        tab = dict(Train_Loss = train_losses[info['best_epoch']], \n",
        "                   Val_Loss   = val_losses[info['best_epoch']],\n",
        "                   Train_Acc  = train_accs[info['best_epoch']],\n",
        "                   Val_Acc    = val_accs[info['best_epoch']],   \n",
        "                   Test_Acc   = test_acc, \n",
        "                   Epoch      = info['best_epoch'] + 1) \n",
        "        \n",
        "        subjectIndexx = subjectIndex + 1\n",
        "        table.loc[subjectIndexx] = tab\n",
        "        \n",
        "        results[subjectIndexx] = dict(train_losses = train_losses, val_losses = val_losses,\n",
        "                                    train_accs = train_accs,     val_accs =  val_accs,                                \n",
        "                                    ytrain = info['ytrain'],     yval= info['yval'])      \n",
        "          \n",
        "        # save models   \n",
        "        fname = 'model_bauyrzhan' + iname + 'S'+ str(subjectIndexx) + '_CNNLSTM_model_'\n",
        "        torch.save(best_model.state_dict(), fname) \n",
        "         \n",
        "        print('::: saving subject {} ::: \\n {}'.format(subjectIndexx, table))         \n",
        "        result_lstm_subspe = dict(table = table, results = results)          \n",
        "          \n",
        "    fname = 'model_bauyrzhan' + iname + '__S'+str(subjectIndexx)  + '_CNNLSTM_subspe_results'\n",
        "    \n",
        "    with open(fname, 'wb') as fp:\n",
        "        pickle.dump(result_lstm_subspe, fp)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "working with data_NUsubjects.pickle\n",
            "Loading subjects: 0\n",
            "Loading subjects: 1\n",
            "Loading subjects: 2\n",
            "Loading subjects: 3\n",
            "Loading subjects: 4\n",
            "Loading subjects: 5\n",
            "Loading subjects: 6\n",
            "Loading subjects: 7\n",
            "Loading subjects: 8\n",
            "Loading subjects: 9\n",
            "Loading subjects: 10\n",
            "dict_keys(['xtrain', 'xvalid', 'xtest', 'ytrain', 'yvalid', 'ytest'])\n",
            "Input data shape torch.Size([4732, 1, 16, 76])\n",
            "Input data shape torch.Size([4629, 1, 16, 76])\n",
            "Input data shape torch.Size([2435, 1, 16, 76])\n",
            "Input data shape torch.Size([7727, 1, 16, 76])\n",
            "Input data shape torch.Size([4026, 1, 16, 76])\n",
            "Input data shape torch.Size([4374, 1, 16, 76])\n",
            "Input data shape torch.Size([1302, 1, 16, 76])\n",
            "Input data shape torch.Size([1393, 1, 16, 76])\n",
            "Input data shape torch.Size([4910, 1, 16, 76])\n",
            "Input data shape torch.Size([4449, 1, 16, 76])\n",
            "Input data shape torch.Size([4458, 1, 16, 76])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "train loss: 0.0870, acc: 0.5325\n",
            "val loss: 0.0868, acc: 0.5376\n",
            "Epoch 2/30\n",
            "train loss: 0.0862, acc: 0.5306\n",
            "val loss: 0.0869, acc: 0.4734\n",
            "Epoch 3/30\n",
            "train loss: 0.0864, acc: 0.5473\n",
            "val loss: 0.0855, acc: 0.5680\n",
            "Epoch 4/30\n",
            "train loss: 0.0866, acc: 0.5338\n",
            "val loss: 0.0861, acc: 0.5385\n",
            "Epoch 5/30\n",
            "train loss: 0.0865, acc: 0.5374\n",
            "val loss: 0.0867, acc: 0.5385\n",
            "Epoch 6/30\n",
            "train loss: 0.0868, acc: 0.5256\n",
            "val loss: 0.0870, acc: 0.5089\n",
            "Epoch 7/30\n",
            "train loss: 0.0869, acc: 0.5311\n",
            "val loss: 0.0902, acc: 0.4607\n",
            "Epoch 8/30\n",
            "train loss: 0.0868, acc: 0.5281\n",
            "val loss: 0.0863, acc: 0.5385\n",
            "Epoch 9/30\n",
            "train loss: 0.0867, acc: 0.5357\n",
            "val loss: 0.0865, acc: 0.5385\n",
            "Epoch 10/30\n",
            "train loss: 0.0869, acc: 0.5237\n",
            "val loss: 0.0866, acc: 0.5376\n",
            "Epoch 11/30\n",
            "train loss: 0.0869, acc: 0.5370\n",
            "val loss: 0.0863, acc: 0.5385\n",
            "Epoch 12/30\n",
            "train loss: 0.0867, acc: 0.5313\n",
            "val loss: 0.0866, acc: 0.5385\n",
            "Epoch 13/30\n",
            "train loss: 0.0868, acc: 0.5254\n",
            "val loss: 0.0864, acc: 0.5385\n",
            "Epoch 14/30\n",
            "train loss: 0.0868, acc: 0.5351\n",
            "val loss: 0.0869, acc: 0.5097\n",
            "Epoch 15/30\n",
            "train loss: 0.0869, acc: 0.5315\n",
            "val loss: 0.0863, acc: 0.5452\n",
            "Epoch 16/30\n",
            "train loss: 0.0864, acc: 0.5414\n",
            "val loss: 0.0866, acc: 0.5385\n",
            "Epoch 17/30\n",
            "train loss: 0.0867, acc: 0.5370\n",
            "val loss: 0.0861, acc: 0.5385\n",
            "Epoch 18/30\n",
            "train loss: 0.0866, acc: 0.5363\n",
            "val loss: 0.0866, acc: 0.5021\n",
            "Epoch 19/30\n",
            "train loss: 0.0867, acc: 0.5351\n",
            "val loss: 0.0862, acc: 0.5385\n",
            "Epoch 20/30\n",
            "train loss: 0.0864, acc: 0.5482\n",
            "val loss: 0.0863, acc: 0.5385\n",
            "Epoch 21/30\n",
            "train loss: 0.0873, acc: 0.5357\n",
            "val loss: 0.0868, acc: 0.5385\n",
            "Epoch 22/30\n",
            "train loss: 0.0867, acc: 0.5292\n",
            "val loss: 0.0865, acc: 0.5385\n",
            "Epoch 23/30\n",
            "train loss: 0.0867, acc: 0.5359\n",
            "val loss: 0.0865, acc: 0.5385\n",
            "Epoch 24/30\n",
            "train loss: 0.0867, acc: 0.5374\n",
            "val loss: 0.0866, acc: 0.5385\n",
            "Epoch 25/30\n",
            "train loss: 0.0869, acc: 0.5298\n",
            "val loss: 0.0877, acc: 0.5385\n",
            "Epoch 26/30\n",
            "train loss: 0.0867, acc: 0.5279\n",
            "val loss: 0.0866, acc: 0.5385\n",
            "Epoch 27/30\n",
            "train loss: 0.0868, acc: 0.5380\n",
            "val loss: 0.0866, acc: 0.5385\n",
            "Epoch 28/30\n",
            "train loss: 0.0868, acc: 0.5383\n",
            "val loss: 0.0874, acc: 0.4725\n",
            "Epoch 29/30\n",
            "train loss: 0.0868, acc: 0.5427\n",
            "val loss: 0.0861, acc: 0.5452\n",
            "Epoch 30/30\n",
            "train loss: 0.0867, acc: 0.5264\n",
            "val loss: 0.0863, acc: 0.5385\n",
            "Training complete in 2m 4s\n",
            "Best val Acc: 0.568047\n",
            "Best Epoch : 3\n",
            "Test Accuracy : 0.2998745294855709\n",
            "::: saving subject 1 ::: \n",
            "    Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1    0.086358   0.08546   0.547337  0.568047  0.299875     3\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0826, acc: 0.6103\n",
            "val loss: 0.0926, acc: 0.5866\n",
            "Epoch 2/30\n",
            "train loss: 0.0773, acc: 0.6658\n",
            "val loss: 0.0876, acc: 0.6253\n",
            "Epoch 3/30\n",
            "train loss: 0.0736, acc: 0.6874\n",
            "val loss: 0.0889, acc: 0.6382\n",
            "Epoch 4/30\n",
            "train loss: 0.0728, acc: 0.6922\n",
            "val loss: 0.0759, acc: 0.6667\n",
            "Epoch 5/30\n",
            "train loss: 0.0713, acc: 0.7027\n",
            "val loss: 0.0786, acc: 0.6374\n",
            "Epoch 6/30\n",
            "train loss: 0.0700, acc: 0.7081\n",
            "val loss: 0.0848, acc: 0.5978\n",
            "Epoch 7/30\n",
            "train loss: 0.0670, acc: 0.7313\n",
            "val loss: 0.0806, acc: 0.6374\n",
            "Epoch 8/30\n",
            "train loss: 0.0639, acc: 0.7552\n",
            "val loss: 0.0822, acc: 0.6615\n",
            "Epoch 9/30\n",
            "train loss: 0.0626, acc: 0.7613\n",
            "val loss: 0.0789, acc: 0.6581\n",
            "Epoch 10/30\n",
            "train loss: 0.0578, acc: 0.7853\n",
            "val loss: 0.0794, acc: 0.6667\n",
            "Epoch 11/30\n",
            "train loss: 0.0559, acc: 0.7930\n",
            "val loss: 0.0844, acc: 0.6675\n",
            "Epoch 12/30\n",
            "train loss: 0.0527, acc: 0.8105\n",
            "val loss: 0.1052, acc: 0.6563\n",
            "Epoch 13/30\n",
            "train loss: 0.0523, acc: 0.8190\n",
            "val loss: 0.0850, acc: 0.6460\n",
            "Epoch 14/30\n",
            "train loss: 0.0487, acc: 0.8315\n",
            "val loss: 0.0912, acc: 0.6753\n",
            "Epoch 15/30\n",
            "train loss: 0.0456, acc: 0.8432\n",
            "val loss: 0.0927, acc: 0.6615\n",
            "Epoch 16/30\n",
            "train loss: 0.0427, acc: 0.8507\n",
            "val loss: 0.1123, acc: 0.6339\n",
            "Epoch 17/30\n",
            "train loss: 0.0419, acc: 0.8572\n",
            "val loss: 0.1010, acc: 0.6701\n",
            "Epoch 18/30\n",
            "train loss: 0.0388, acc: 0.8771\n",
            "val loss: 0.1524, acc: 0.6494\n",
            "Epoch 19/30\n",
            "train loss: 0.0405, acc: 0.8717\n",
            "val loss: 0.1351, acc: 0.6417\n",
            "Epoch 20/30\n",
            "train loss: 0.0366, acc: 0.8766\n",
            "val loss: 0.1404, acc: 0.6400\n",
            "Epoch 21/30\n",
            "train loss: 0.0361, acc: 0.8849\n",
            "val loss: 0.1410, acc: 0.5943\n",
            "Epoch 22/30\n",
            "train loss: 0.0359, acc: 0.8801\n",
            "val loss: 0.1121, acc: 0.6391\n",
            "Epoch 23/30\n",
            "train loss: 0.0334, acc: 0.8954\n",
            "val loss: 0.1563, acc: 0.6555\n",
            "Epoch 24/30\n",
            "train loss: 0.0316, acc: 0.9015\n",
            "val loss: 0.1495, acc: 0.6572\n",
            "Epoch 25/30\n",
            "train loss: 0.0329, acc: 0.8995\n",
            "val loss: 0.1405, acc: 0.6400\n",
            "Epoch 26/30\n",
            "train loss: 0.0302, acc: 0.9043\n",
            "val loss: 0.1726, acc: 0.6124\n",
            "Epoch 27/30\n",
            "train loss: 0.0302, acc: 0.9037\n",
            "val loss: 0.1387, acc: 0.6296\n",
            "Epoch 28/30\n",
            "train loss: 0.0308, acc: 0.9134\n",
            "val loss: 0.1522, acc: 0.6400\n",
            "Epoch 29/30\n",
            "train loss: 0.0289, acc: 0.9142\n",
            "val loss: 0.1307, acc: 0.6546\n",
            "Epoch 30/30\n",
            "train loss: 0.0320, acc: 0.9019\n",
            "val loss: 0.1768, acc: 0.5952\n",
            "Training complete in 2m 2s\n",
            "Best val Acc: 0.675280\n",
            "Best Epoch : 14\n",
            "Test Accuracy : 0.6530612244897959\n",
            "::: saving subject 2 ::: \n",
            "    Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1    0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2    0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0869, acc: 0.5310\n",
            "val loss: 0.0854, acc: 0.5876\n",
            "Epoch 2/30\n",
            "train loss: 0.0873, acc: 0.5244\n",
            "val loss: 0.0888, acc: 0.4583\n",
            "Epoch 3/30\n",
            "train loss: 0.0868, acc: 0.5351\n",
            "val loss: 0.0857, acc: 0.5401\n",
            "Epoch 4/30\n",
            "train loss: 0.0866, acc: 0.5417\n",
            "val loss: 0.0864, acc: 0.5385\n",
            "Epoch 5/30\n",
            "train loss: 0.0860, acc: 0.5478\n",
            "val loss: 0.0861, acc: 0.5597\n",
            "Epoch 6/30\n",
            "train loss: 0.0867, acc: 0.5384\n",
            "val loss: 0.0904, acc: 0.4288\n",
            "Epoch 7/30\n",
            "train loss: 0.0869, acc: 0.5302\n",
            "val loss: 0.0869, acc: 0.5385\n",
            "Epoch 8/30\n",
            "train loss: 0.0869, acc: 0.5273\n",
            "val loss: 0.0868, acc: 0.5434\n",
            "Epoch 9/30\n",
            "train loss: 0.0868, acc: 0.5433\n",
            "val loss: 0.0878, acc: 0.4992\n",
            "Epoch 10/30\n",
            "train loss: 0.0866, acc: 0.5503\n",
            "val loss: 0.0872, acc: 0.5516\n",
            "Epoch 11/30\n",
            "train loss: 0.0872, acc: 0.5347\n",
            "val loss: 0.0874, acc: 0.5254\n",
            "Epoch 12/30\n",
            "train loss: 0.0874, acc: 0.5343\n",
            "val loss: 0.0895, acc: 0.4615\n",
            "Epoch 13/30\n",
            "train loss: 0.0869, acc: 0.5331\n",
            "val loss: 0.0880, acc: 0.5385\n",
            "Epoch 14/30\n",
            "train loss: 0.0869, acc: 0.5277\n",
            "val loss: 0.0877, acc: 0.4992\n",
            "Epoch 15/30\n",
            "train loss: 0.0868, acc: 0.5343\n",
            "val loss: 0.0874, acc: 0.5385\n",
            "Epoch 16/30\n",
            "train loss: 0.0866, acc: 0.5384\n",
            "val loss: 0.0870, acc: 0.5401\n",
            "Epoch 17/30\n",
            "train loss: 0.0870, acc: 0.5248\n",
            "val loss: 0.0873, acc: 0.5319\n",
            "Epoch 18/30\n",
            "train loss: 0.0870, acc: 0.5298\n",
            "val loss: 0.0869, acc: 0.5303\n",
            "Epoch 19/30\n",
            "train loss: 0.0870, acc: 0.5446\n",
            "val loss: 0.0870, acc: 0.5385\n",
            "Epoch 20/30\n",
            "train loss: 0.0866, acc: 0.5380\n",
            "val loss: 0.0870, acc: 0.5385\n",
            "Epoch 21/30\n",
            "train loss: 0.0867, acc: 0.5343\n",
            "val loss: 0.0861, acc: 0.5974\n",
            "Epoch 22/30\n",
            "train loss: 0.0868, acc: 0.5224\n",
            "val loss: 0.0871, acc: 0.5303\n",
            "Epoch 23/30\n",
            "train loss: 0.0870, acc: 0.5199\n",
            "val loss: 0.0871, acc: 0.5401\n",
            "Epoch 24/30\n",
            "train loss: 0.0868, acc: 0.5441\n",
            "val loss: 0.0868, acc: 0.5434\n",
            "Epoch 25/30\n",
            "train loss: 0.0867, acc: 0.5335\n",
            "val loss: 0.0869, acc: 0.5597\n",
            "Epoch 26/30\n",
            "train loss: 0.0868, acc: 0.5248\n",
            "val loss: 0.0878, acc: 0.5221\n",
            "Epoch 27/30\n",
            "train loss: 0.0865, acc: 0.5306\n",
            "val loss: 0.0904, acc: 0.4926\n",
            "Epoch 28/30\n",
            "train loss: 0.0871, acc: 0.5265\n",
            "val loss: 0.0868, acc: 0.5385\n",
            "Epoch 29/30\n",
            "train loss: 0.0862, acc: 0.5507\n",
            "val loss: 0.0893, acc: 0.5499\n",
            "Epoch 30/30\n",
            "train loss: 0.0867, acc: 0.5400\n",
            "val loss: 0.0860, acc: 0.5516\n",
            "Training complete in 1m 5s\n",
            "Best val Acc: 0.597381\n",
            "Best Epoch : 21\n",
            "Test Accuracy : 0.25060827250608275\n",
            "::: saving subject 3 ::: \n",
            "    Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1    0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2    0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "3    0.086747  0.086076   0.534292  0.597381  0.250608    21\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0770, acc: 0.6537\n",
            "val loss: 0.0726, acc: 0.7007\n",
            "Epoch 2/30\n",
            "train loss: 0.0674, acc: 0.7370\n",
            "val loss: 0.0771, acc: 0.6712\n",
            "Epoch 3/30\n",
            "train loss: 0.0651, acc: 0.7416\n",
            "val loss: 0.0733, acc: 0.7105\n",
            "Epoch 4/30\n",
            "train loss: 0.0640, acc: 0.7577\n",
            "val loss: 0.0722, acc: 0.7048\n",
            "Epoch 5/30\n",
            "train loss: 0.0625, acc: 0.7611\n",
            "val loss: 0.0704, acc: 0.7105\n",
            "Epoch 6/30\n",
            "train loss: 0.0609, acc: 0.7705\n",
            "val loss: 0.0735, acc: 0.6779\n",
            "Epoch 7/30\n",
            "train loss: 0.0616, acc: 0.7677\n",
            "val loss: 0.0730, acc: 0.7188\n",
            "Epoch 8/30\n",
            "train loss: 0.0601, acc: 0.7713\n",
            "val loss: 0.0741, acc: 0.7002\n",
            "Epoch 9/30\n",
            "train loss: 0.0582, acc: 0.7845\n",
            "val loss: 0.0727, acc: 0.7204\n",
            "Epoch 10/30\n",
            "train loss: 0.0580, acc: 0.7870\n",
            "val loss: 0.0782, acc: 0.7017\n",
            "Epoch 11/30\n",
            "train loss: 0.0576, acc: 0.7858\n",
            "val loss: 0.0927, acc: 0.6758\n",
            "Epoch 12/30\n",
            "train loss: 0.0549, acc: 0.7984\n",
            "val loss: 0.0831, acc: 0.7105\n",
            "Epoch 13/30\n",
            "train loss: 0.0552, acc: 0.7990\n",
            "val loss: 0.0883, acc: 0.7007\n",
            "Epoch 14/30\n",
            "train loss: 0.0548, acc: 0.7959\n",
            "val loss: 0.0852, acc: 0.6903\n",
            "Epoch 15/30\n",
            "train loss: 0.0539, acc: 0.8051\n",
            "val loss: 0.0746, acc: 0.7152\n",
            "Epoch 16/30\n",
            "train loss: 0.0534, acc: 0.8087\n",
            "val loss: 0.0912, acc: 0.7064\n",
            "Epoch 17/30\n",
            "train loss: 0.0525, acc: 0.8130\n",
            "val loss: 0.0947, acc: 0.7095\n",
            "Epoch 18/30\n",
            "train loss: 0.0516, acc: 0.8157\n",
            "val loss: 0.0796, acc: 0.7157\n",
            "Epoch 19/30\n",
            "train loss: 0.0529, acc: 0.8051\n",
            "val loss: 0.0858, acc: 0.6737\n",
            "Epoch 20/30\n",
            "train loss: 0.0512, acc: 0.8195\n",
            "val loss: 0.0890, acc: 0.6727\n",
            "Epoch 21/30\n",
            "train loss: 0.0502, acc: 0.8213\n",
            "val loss: 0.0943, acc: 0.7198\n",
            "Epoch 22/30\n",
            "train loss: 0.0490, acc: 0.8275\n",
            "val loss: 0.0937, acc: 0.6825\n",
            "Epoch 23/30\n",
            "train loss: 0.0509, acc: 0.8222\n",
            "val loss: 0.0820, acc: 0.7136\n",
            "Epoch 24/30\n",
            "train loss: 0.0486, acc: 0.8299\n",
            "val loss: 0.0951, acc: 0.6981\n",
            "Epoch 25/30\n",
            "train loss: 0.0492, acc: 0.8283\n",
            "val loss: 0.1001, acc: 0.6722\n",
            "Epoch 26/30\n",
            "train loss: 0.0467, acc: 0.8407\n",
            "val loss: 0.1051, acc: 0.6862\n",
            "Epoch 27/30\n",
            "train loss: 0.0459, acc: 0.8443\n",
            "val loss: 0.1030, acc: 0.6950\n",
            "Epoch 28/30\n",
            "train loss: 0.0441, acc: 0.8508\n",
            "val loss: 0.1169, acc: 0.7095\n",
            "Epoch 29/30\n",
            "train loss: 0.0459, acc: 0.8406\n",
            "val loss: 0.0830, acc: 0.7095\n",
            "Epoch 30/30\n",
            "train loss: 0.0459, acc: 0.8444\n",
            "val loss: 0.1220, acc: 0.7033\n",
            "Training complete in 3m 22s\n",
            "Best val Acc: 0.720352\n",
            "Best Epoch : 9\n",
            "Test Accuracy : 0.6940184049079755\n",
            "::: saving subject 4 ::: \n",
            "    Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1    0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2    0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "3    0.086747  0.086076   0.534292  0.597381  0.250608    21\n",
            "4    0.058172  0.072711   0.784522  0.720352  0.694018     9\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0689, acc: 0.7089\n",
            "val loss: 0.0641, acc: 0.7757\n",
            "Epoch 2/30\n",
            "train loss: 0.0546, acc: 0.8050\n",
            "val loss: 0.0557, acc: 0.8004\n",
            "Epoch 3/30\n",
            "train loss: 0.0483, acc: 0.8286\n",
            "val loss: 0.0606, acc: 0.7628\n",
            "Epoch 4/30\n",
            "train loss: 0.0430, acc: 0.8537\n",
            "val loss: 0.0723, acc: 0.7806\n",
            "Epoch 5/30\n",
            "train loss: 0.0391, acc: 0.8726\n",
            "val loss: 0.0743, acc: 0.7480\n",
            "Epoch 6/30\n",
            "train loss: 0.0370, acc: 0.8770\n",
            "val loss: 0.0681, acc: 0.7935\n",
            "Epoch 7/30\n",
            "train loss: 0.0360, acc: 0.8880\n",
            "val loss: 0.0650, acc: 0.7945\n",
            "Epoch 8/30\n",
            "train loss: 0.0325, acc: 0.8979\n",
            "val loss: 0.0976, acc: 0.7757\n",
            "Epoch 9/30\n",
            "train loss: 0.0327, acc: 0.9004\n",
            "val loss: 0.0801, acc: 0.7648\n",
            "Epoch 10/30\n",
            "train loss: 0.0306, acc: 0.9039\n",
            "val loss: 0.0818, acc: 0.7569\n",
            "Epoch 11/30\n",
            "train loss: 0.0304, acc: 0.9044\n",
            "val loss: 0.0750, acc: 0.7767\n",
            "Epoch 12/30\n",
            "train loss: 0.0297, acc: 0.9136\n",
            "val loss: 0.1040, acc: 0.7530\n",
            "Epoch 13/30\n",
            "train loss: 0.0275, acc: 0.9146\n",
            "val loss: 0.1291, acc: 0.7184\n",
            "Epoch 14/30\n",
            "train loss: 0.0288, acc: 0.9111\n",
            "val loss: 0.0612, acc: 0.7984\n",
            "Epoch 15/30\n",
            "train loss: 0.0264, acc: 0.9178\n",
            "val loss: 0.1079, acc: 0.7470\n",
            "Epoch 16/30\n",
            "train loss: 0.0254, acc: 0.9205\n",
            "val loss: 0.0930, acc: 0.7628\n",
            "Epoch 17/30\n",
            "train loss: 0.0253, acc: 0.9208\n",
            "val loss: 0.1511, acc: 0.7223\n",
            "Epoch 18/30\n",
            "train loss: 0.0251, acc: 0.9228\n",
            "val loss: 0.0956, acc: 0.7372\n",
            "Epoch 19/30\n",
            "train loss: 0.0248, acc: 0.9215\n",
            "val loss: 0.1426, acc: 0.6848\n",
            "Epoch 20/30\n",
            "train loss: 0.0222, acc: 0.9324\n",
            "val loss: 0.1203, acc: 0.7006\n",
            "Epoch 21/30\n",
            "train loss: 0.0245, acc: 0.9272\n",
            "val loss: 0.0770, acc: 0.7885\n",
            "Epoch 22/30\n",
            "train loss: 0.0210, acc: 0.9359\n",
            "val loss: 0.1044, acc: 0.7767\n",
            "Epoch 23/30\n",
            "train loss: 0.0250, acc: 0.9218\n",
            "val loss: 0.0756, acc: 0.7875\n",
            "Epoch 24/30\n",
            "train loss: 0.0212, acc: 0.9329\n",
            "val loss: 0.0887, acc: 0.7787\n",
            "Epoch 25/30\n",
            "train loss: 0.0215, acc: 0.9362\n",
            "val loss: 0.0994, acc: 0.7619\n",
            "Epoch 26/30\n",
            "train loss: 0.0222, acc: 0.9359\n",
            "val loss: 0.1161, acc: 0.7549\n",
            "Epoch 27/30\n",
            "train loss: 0.0222, acc: 0.9287\n",
            "val loss: 0.0673, acc: 0.7994\n",
            "Epoch 28/30\n",
            "train loss: 0.0235, acc: 0.9307\n",
            "val loss: 0.1090, acc: 0.7520\n",
            "Epoch 29/30\n",
            "train loss: 0.0212, acc: 0.9394\n",
            "val loss: 0.1232, acc: 0.7658\n",
            "Epoch 30/30\n",
            "train loss: 0.0251, acc: 0.9262\n",
            "val loss: 0.1049, acc: 0.7194\n",
            "Training complete in 1m 45s\n",
            "Best val Acc: 0.800395\n",
            "Best Epoch : 2\n",
            "Test Accuracy : 0.8191176470588235\n",
            "::: saving subject 5 ::: \n",
            "    Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1    0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2    0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "3    0.086747  0.086076   0.534292  0.597381  0.250608    21\n",
            "4    0.058172  0.072711   0.784522  0.720352  0.694018     9\n",
            "5    0.054566  0.055684   0.805017  0.800395  0.819118     2\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0831, acc: 0.6015\n",
            "val loss: 0.0823, acc: 0.6370\n",
            "Epoch 2/30\n",
            "train loss: 0.0809, acc: 0.6399\n",
            "val loss: 0.0829, acc: 0.5921\n",
            "Epoch 3/30\n",
            "train loss: 0.0803, acc: 0.6294\n",
            "val loss: 0.0765, acc: 0.6728\n",
            "Epoch 4/30\n",
            "train loss: 0.0809, acc: 0.6241\n",
            "val loss: 0.0778, acc: 0.6398\n",
            "Epoch 5/30\n",
            "train loss: 0.0816, acc: 0.6184\n",
            "val loss: 0.0765, acc: 0.6847\n",
            "Epoch 6/30\n",
            "train loss: 0.0808, acc: 0.6287\n",
            "val loss: 0.0806, acc: 0.6269\n",
            "Epoch 7/30\n",
            "train loss: 0.0809, acc: 0.6161\n",
            "val loss: 0.0771, acc: 0.6599\n",
            "Epoch 8/30\n",
            "train loss: 0.0814, acc: 0.6209\n",
            "val loss: 0.0796, acc: 0.6508\n",
            "Epoch 9/30\n",
            "train loss: 0.0813, acc: 0.6200\n",
            "val loss: 0.0844, acc: 0.5811\n",
            "Epoch 10/30\n",
            "train loss: 0.0817, acc: 0.6177\n",
            "val loss: 0.0797, acc: 0.6279\n",
            "Epoch 11/30\n",
            "train loss: 0.0816, acc: 0.6269\n",
            "val loss: 0.0787, acc: 0.6609\n",
            "Epoch 12/30\n",
            "train loss: 0.0812, acc: 0.6235\n",
            "val loss: 0.0828, acc: 0.6059\n",
            "Epoch 13/30\n",
            "train loss: 0.0810, acc: 0.6232\n",
            "val loss: 0.0775, acc: 0.6838\n",
            "Epoch 14/30\n",
            "train loss: 0.0814, acc: 0.6241\n",
            "val loss: 0.0786, acc: 0.6691\n",
            "Epoch 15/30\n",
            "train loss: 0.0813, acc: 0.6136\n",
            "val loss: 0.0772, acc: 0.6810\n",
            "Epoch 16/30\n",
            "train loss: 0.0814, acc: 0.6239\n",
            "val loss: 0.0785, acc: 0.6581\n",
            "Epoch 17/30\n",
            "train loss: 0.0814, acc: 0.6221\n",
            "val loss: 0.0781, acc: 0.6590\n",
            "Epoch 18/30\n",
            "train loss: 0.0814, acc: 0.6214\n",
            "val loss: 0.0799, acc: 0.6141\n",
            "Epoch 19/30\n",
            "train loss: 0.0822, acc: 0.6045\n",
            "val loss: 0.0777, acc: 0.6664\n",
            "Epoch 20/30\n",
            "train loss: 0.0827, acc: 0.6031\n",
            "val loss: 0.0796, acc: 0.6609\n",
            "Epoch 21/30\n",
            "train loss: 0.0821, acc: 0.6086\n",
            "val loss: 0.0821, acc: 0.6132\n",
            "Epoch 22/30\n",
            "train loss: 0.0820, acc: 0.6134\n",
            "val loss: 0.0836, acc: 0.5628\n",
            "Epoch 23/30\n",
            "train loss: 0.0816, acc: 0.6113\n",
            "val loss: 0.0743, acc: 0.6984\n",
            "Epoch 24/30\n",
            "train loss: 0.0812, acc: 0.6251\n",
            "val loss: 0.0874, acc: 0.5234\n",
            "Epoch 25/30\n",
            "train loss: 0.0813, acc: 0.6136\n",
            "val loss: 0.0799, acc: 0.6453\n",
            "Epoch 26/30\n",
            "train loss: 0.0812, acc: 0.6187\n",
            "val loss: 0.0774, acc: 0.6517\n",
            "Epoch 27/30\n",
            "train loss: 0.0808, acc: 0.6248\n",
            "val loss: 0.0854, acc: 0.5582\n",
            "Epoch 28/30\n",
            "train loss: 0.0997, acc: 0.5681\n",
            "val loss: 0.0778, acc: 0.6957\n",
            "Epoch 29/30\n",
            "train loss: 0.0815, acc: 0.6136\n",
            "val loss: 0.0855, acc: 0.5857\n",
            "Epoch 30/30\n",
            "train loss: 0.0817, acc: 0.6123\n",
            "val loss: 0.0798, acc: 0.6343\n",
            "Training complete in 1m 55s\n",
            "Best val Acc: 0.698442\n",
            "Best Epoch : 23\n",
            "Test Accuracy : 0.7144654088050314\n",
            "::: saving subject 6 ::: \n",
            "    Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1    0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2    0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "3    0.086747  0.086076   0.534292  0.597381  0.250608    21\n",
            "4    0.058172  0.072711   0.784522  0.720352  0.694018     9\n",
            "5    0.054566  0.055684   0.805017  0.800395  0.819118     2\n",
            "6    0.081580  0.074281   0.611340  0.698442  0.714465    23\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0709, acc: 0.7227\n",
            "val loss: 0.0596, acc: 0.7920\n",
            "Epoch 2/30\n",
            "train loss: 0.0531, acc: 0.8172\n",
            "val loss: 0.0710, acc: 0.7982\n",
            "Epoch 3/30\n",
            "train loss: 0.0451, acc: 0.8495\n",
            "val loss: 0.0642, acc: 0.7982\n",
            "Epoch 4/30\n",
            "train loss: 0.0401, acc: 0.8779\n",
            "val loss: 0.0689, acc: 0.8226\n",
            "Epoch 5/30\n",
            "train loss: 0.0350, acc: 0.8879\n",
            "val loss: 0.0670, acc: 0.8471\n",
            "Epoch 6/30\n",
            "train loss: 0.0340, acc: 0.8978\n",
            "val loss: 0.0724, acc: 0.8226\n",
            "Epoch 7/30\n",
            "train loss: 0.0294, acc: 0.9086\n",
            "val loss: 0.0789, acc: 0.8196\n",
            "Epoch 8/30\n",
            "train loss: 0.0258, acc: 0.9270\n",
            "val loss: 0.0841, acc: 0.8257\n",
            "Epoch 9/30\n",
            "train loss: 0.0305, acc: 0.9063\n",
            "val loss: 0.0893, acc: 0.8196\n",
            "Epoch 10/30\n",
            "train loss: 0.0262, acc: 0.9224\n",
            "val loss: 0.0812, acc: 0.7737\n",
            "Epoch 11/30\n",
            "train loss: 0.0239, acc: 0.9332\n",
            "val loss: 0.0895, acc: 0.7920\n",
            "Epoch 12/30\n",
            "train loss: 0.0214, acc: 0.9355\n",
            "val loss: 0.0862, acc: 0.7859\n",
            "Epoch 13/30\n",
            "train loss: 0.0198, acc: 0.9378\n",
            "val loss: 0.0709, acc: 0.8318\n",
            "Epoch 14/30\n",
            "train loss: 0.0212, acc: 0.9386\n",
            "val loss: 0.1381, acc: 0.7095\n",
            "Epoch 15/30\n",
            "train loss: 0.0239, acc: 0.9309\n",
            "val loss: 0.0943, acc: 0.7462\n",
            "Epoch 16/30\n",
            "train loss: 0.0177, acc: 0.9393\n",
            "val loss: 0.0660, acc: 0.8073\n",
            "Epoch 17/30\n",
            "train loss: 0.0154, acc: 0.9508\n",
            "val loss: 0.0677, acc: 0.8440\n",
            "Epoch 18/30\n",
            "train loss: 0.0183, acc: 0.9501\n",
            "val loss: 0.0623, acc: 0.8349\n",
            "Epoch 19/30\n",
            "train loss: 0.0177, acc: 0.9570\n",
            "val loss: 0.1018, acc: 0.7676\n",
            "Epoch 20/30\n",
            "train loss: 0.0190, acc: 0.9439\n",
            "val loss: 0.1138, acc: 0.7401\n",
            "Epoch 21/30\n",
            "train loss: 0.0170, acc: 0.9455\n",
            "val loss: 0.0837, acc: 0.7982\n",
            "Epoch 22/30\n",
            "train loss: 0.0157, acc: 0.9524\n",
            "val loss: 0.1101, acc: 0.7584\n",
            "Epoch 23/30\n",
            "train loss: 0.0109, acc: 0.9739\n",
            "val loss: 0.1094, acc: 0.7462\n",
            "Epoch 24/30\n",
            "train loss: 0.0145, acc: 0.9593\n",
            "val loss: 0.1079, acc: 0.7737\n",
            "Epoch 25/30\n",
            "train loss: 0.0139, acc: 0.9601\n",
            "val loss: 0.0787, acc: 0.8226\n",
            "Epoch 26/30\n",
            "train loss: 0.0098, acc: 0.9739\n",
            "val loss: 0.0976, acc: 0.7859\n",
            "Epoch 27/30\n",
            "train loss: 0.0110, acc: 0.9716\n",
            "val loss: 0.0860, acc: 0.8196\n",
            "Epoch 28/30\n",
            "train loss: 0.0150, acc: 0.9516\n",
            "val loss: 0.0733, acc: 0.8869\n",
            "Epoch 29/30\n",
            "train loss: 0.0117, acc: 0.9639\n",
            "val loss: 0.0945, acc: 0.8012\n",
            "Epoch 30/30\n",
            "train loss: 0.0126, acc: 0.9693\n",
            "val loss: 0.0951, acc: 0.8012\n",
            "Training complete in 0m 35s\n",
            "Best val Acc: 0.886850\n",
            "Best Epoch : 28\n",
            "Test Accuracy : 0.8626609442060086\n",
            "::: saving subject 7 ::: \n",
            "    Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1    0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2    0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "3    0.086747  0.086076   0.534292  0.597381  0.250608    21\n",
            "4    0.058172  0.072711   0.784522  0.720352  0.694018     9\n",
            "5    0.054566  0.055684   0.805017  0.800395  0.819118     2\n",
            "6    0.081580  0.074281   0.611340  0.698442  0.714465    23\n",
            "7    0.015023  0.073269   0.951613  0.886850  0.862661    28\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0869, acc: 0.5441\n",
            "val loss: 0.0855, acc: 0.5886\n",
            "Epoch 2/30\n",
            "train loss: 0.0798, acc: 0.6346\n",
            "val loss: 0.0807, acc: 0.6029\n",
            "Epoch 3/30\n",
            "train loss: 0.0698, acc: 0.7215\n",
            "val loss: 0.0874, acc: 0.6286\n",
            "Epoch 4/30\n",
            "train loss: 0.0576, acc: 0.7983\n",
            "val loss: 0.1049, acc: 0.6286\n",
            "Epoch 5/30\n",
            "train loss: 0.0517, acc: 0.8234\n",
            "val loss: 0.1268, acc: 0.6000\n",
            "Epoch 6/30\n",
            "train loss: 0.0414, acc: 0.8708\n",
            "val loss: 0.1083, acc: 0.6057\n",
            "Epoch 7/30\n",
            "train loss: 0.0316, acc: 0.9038\n",
            "val loss: 0.1739, acc: 0.5686\n",
            "Epoch 8/30\n",
            "train loss: 0.0259, acc: 0.9153\n",
            "val loss: 0.1825, acc: 0.5686\n",
            "Epoch 9/30\n",
            "train loss: 0.0245, acc: 0.9296\n",
            "val loss: 0.1864, acc: 0.6200\n",
            "Epoch 10/30\n",
            "train loss: 0.0176, acc: 0.9562\n",
            "val loss: 0.2083, acc: 0.6143\n",
            "Epoch 11/30\n",
            "train loss: 0.0313, acc: 0.9017\n",
            "val loss: 0.2155, acc: 0.6029\n",
            "Epoch 12/30\n",
            "train loss: 0.0198, acc: 0.9411\n",
            "val loss: 0.1552, acc: 0.6400\n",
            "Epoch 13/30\n",
            "train loss: 0.0225, acc: 0.9368\n",
            "val loss: 0.2529, acc: 0.5343\n",
            "Epoch 14/30\n",
            "train loss: 0.0164, acc: 0.9505\n",
            "val loss: 0.2841, acc: 0.5743\n",
            "Epoch 15/30\n",
            "train loss: 0.0117, acc: 0.9698\n",
            "val loss: 0.2533, acc: 0.5543\n",
            "Epoch 16/30\n",
            "train loss: 0.0175, acc: 0.9476\n",
            "val loss: 0.2385, acc: 0.5829\n",
            "Epoch 17/30\n",
            "train loss: 0.0086, acc: 0.9799\n",
            "val loss: 0.2682, acc: 0.5686\n",
            "Epoch 18/30\n",
            "train loss: 0.0111, acc: 0.9713\n",
            "val loss: 0.2086, acc: 0.6343\n",
            "Epoch 19/30\n",
            "train loss: 0.0091, acc: 0.9713\n",
            "val loss: 0.3363, acc: 0.5543\n",
            "Epoch 20/30\n",
            "train loss: 0.0130, acc: 0.9584\n",
            "val loss: 0.3185, acc: 0.5743\n",
            "Epoch 21/30\n",
            "train loss: 0.0115, acc: 0.9627\n",
            "val loss: 0.2237, acc: 0.6200\n",
            "Epoch 22/30\n",
            "train loss: 0.0108, acc: 0.9677\n",
            "val loss: 0.3185, acc: 0.5343\n",
            "Epoch 23/30\n",
            "train loss: 0.0143, acc: 0.9541\n",
            "val loss: 0.1477, acc: 0.6086\n",
            "Epoch 24/30\n",
            "train loss: 0.0094, acc: 0.9742\n",
            "val loss: 0.2776, acc: 0.5857\n",
            "Epoch 25/30\n",
            "train loss: 0.0097, acc: 0.9727\n",
            "val loss: 0.3165, acc: 0.5343\n",
            "Epoch 26/30\n",
            "train loss: 0.0094, acc: 0.9727\n",
            "val loss: 0.2154, acc: 0.5886\n",
            "Epoch 27/30\n",
            "train loss: 0.0102, acc: 0.9713\n",
            "val loss: 0.2169, acc: 0.5657\n",
            "Epoch 28/30\n",
            "train loss: 0.0064, acc: 0.9835\n",
            "val loss: 0.3293, acc: 0.5686\n",
            "Epoch 29/30\n",
            "train loss: 0.0165, acc: 0.9584\n",
            "val loss: 0.2175, acc: 0.5657\n",
            "Epoch 30/30\n",
            "train loss: 0.0097, acc: 0.9727\n",
            "val loss: 0.3236, acc: 0.5514\n",
            "Training complete in 0m 37s\n",
            "Best val Acc: 0.640000\n",
            "Best Epoch : 12\n",
            "Test Accuracy : 0.7872340425531915\n",
            "::: saving subject 8 ::: \n",
            "    Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1    0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2    0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "3    0.086747  0.086076   0.534292  0.597381  0.250608    21\n",
            "4    0.058172  0.072711   0.784522  0.720352  0.694018     9\n",
            "5    0.054566  0.055684   0.805017  0.800395  0.819118     2\n",
            "6    0.081580  0.074281   0.611340  0.698442  0.714465    23\n",
            "7    0.015023  0.073269   0.951613  0.886850  0.862661    28\n",
            "8    0.019791  0.155218   0.941134  0.640000  0.787234    12\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0841, acc: 0.5829\n",
            "val loss: 0.0812, acc: 0.6553\n",
            "Epoch 2/30\n",
            "train loss: 0.0797, acc: 0.6448\n",
            "val loss: 0.0819, acc: 0.6464\n",
            "Epoch 3/30\n",
            "train loss: 0.0787, acc: 0.6536\n",
            "val loss: 0.0843, acc: 0.6050\n",
            "Epoch 4/30\n",
            "train loss: 0.0766, acc: 0.6587\n",
            "val loss: 0.0834, acc: 0.6156\n",
            "Epoch 5/30\n",
            "train loss: 0.0768, acc: 0.6688\n",
            "val loss: 0.0766, acc: 0.6796\n",
            "Epoch 6/30\n",
            "train loss: 0.0739, acc: 0.6872\n",
            "val loss: 0.0826, acc: 0.6164\n",
            "Epoch 7/30\n",
            "train loss: 0.0737, acc: 0.6896\n",
            "val loss: 0.0798, acc: 0.6642\n",
            "Epoch 8/30\n",
            "train loss: 0.0729, acc: 0.6988\n",
            "val loss: 0.0829, acc: 0.6164\n",
            "Epoch 9/30\n",
            "train loss: 0.0711, acc: 0.7116\n",
            "val loss: 0.0774, acc: 0.6837\n",
            "Epoch 10/30\n",
            "train loss: 0.0685, acc: 0.7218\n",
            "val loss: 0.0867, acc: 0.6642\n",
            "Epoch 11/30\n",
            "train loss: 0.0678, acc: 0.7310\n",
            "val loss: 0.0770, acc: 0.6821\n",
            "Epoch 12/30\n",
            "train loss: 0.0655, acc: 0.7470\n",
            "val loss: 0.0787, acc: 0.6869\n",
            "Epoch 13/30\n",
            "train loss: 0.0631, acc: 0.7633\n",
            "val loss: 0.0872, acc: 0.6764\n",
            "Epoch 14/30\n",
            "train loss: 0.0612, acc: 0.7656\n",
            "val loss: 0.0863, acc: 0.6650\n",
            "Epoch 15/30\n",
            "train loss: 0.0571, acc: 0.7886\n",
            "val loss: 0.0981, acc: 0.6456\n",
            "Epoch 16/30\n",
            "train loss: 0.0580, acc: 0.7855\n",
            "val loss: 0.1000, acc: 0.6723\n",
            "Epoch 17/30\n",
            "train loss: 0.0553, acc: 0.7955\n",
            "val loss: 0.0859, acc: 0.6399\n",
            "Epoch 18/30\n",
            "train loss: 0.0533, acc: 0.8051\n",
            "val loss: 0.1159, acc: 0.6440\n",
            "Epoch 19/30\n",
            "train loss: 0.0519, acc: 0.8179\n",
            "val loss: 0.1160, acc: 0.6326\n",
            "Epoch 20/30\n",
            "train loss: 0.0503, acc: 0.8257\n",
            "val loss: 0.1095, acc: 0.6561\n",
            "Epoch 21/30\n",
            "train loss: 0.0484, acc: 0.8283\n",
            "val loss: 0.1343, acc: 0.6285\n",
            "Epoch 22/30\n",
            "train loss: 0.0482, acc: 0.8352\n",
            "val loss: 0.1471, acc: 0.6067\n",
            "Epoch 23/30\n",
            "train loss: 0.0463, acc: 0.8444\n",
            "val loss: 0.1500, acc: 0.6253\n",
            "Epoch 24/30\n",
            "train loss: 0.0449, acc: 0.8513\n",
            "val loss: 0.1434, acc: 0.6310\n",
            "Epoch 25/30\n",
            "train loss: 0.0433, acc: 0.8532\n",
            "val loss: 0.1698, acc: 0.6399\n",
            "Epoch 26/30\n",
            "train loss: 0.0430, acc: 0.8544\n",
            "val loss: 0.1069, acc: 0.6586\n",
            "Epoch 27/30\n",
            "train loss: 0.0430, acc: 0.8556\n",
            "val loss: 0.1128, acc: 0.6707\n",
            "Epoch 28/30\n",
            "train loss: 0.0426, acc: 0.8597\n",
            "val loss: 0.1515, acc: 0.6285\n",
            "Epoch 29/30\n",
            "train loss: 0.0390, acc: 0.8719\n",
            "val loss: 0.1437, acc: 0.6626\n",
            "Epoch 30/30\n",
            "train loss: 0.0385, acc: 0.8717\n",
            "val loss: 0.1679, acc: 0.6075\n",
            "Training complete in 2m 9s\n",
            "Best val Acc: 0.686942\n",
            "Best Epoch : 12\n",
            "Test Accuracy : 0.6739130434782609\n",
            "::: saving subject 9 ::: \n",
            "    Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1    0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2    0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "3    0.086747  0.086076   0.534292  0.597381  0.250608    21\n",
            "4    0.058172  0.072711   0.784522  0.720352  0.694018     9\n",
            "5    0.054566  0.055684   0.805017  0.800395  0.819118     2\n",
            "6    0.081580  0.074281   0.611340  0.698442  0.714465    23\n",
            "7    0.015023  0.073269   0.951613  0.886850  0.862661    28\n",
            "8    0.019791  0.155218   0.941134  0.640000  0.787234    12\n",
            "9    0.065477  0.078710   0.747047  0.686942  0.673913    12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "train loss: 0.0877, acc: 0.5118\n",
            "val loss: 0.0872, acc: 0.4987\n",
            "Epoch 2/30\n",
            "train loss: 0.0855, acc: 0.5651\n",
            "val loss: 0.0870, acc: 0.5193\n",
            "Epoch 3/30\n",
            "train loss: 0.0850, acc: 0.5759\n",
            "val loss: 0.0868, acc: 0.5382\n",
            "Epoch 4/30\n",
            "train loss: 0.0843, acc: 0.5817\n",
            "val loss: 0.0882, acc: 0.5310\n",
            "Epoch 5/30\n",
            "train loss: 0.0837, acc: 0.5898\n",
            "val loss: 0.0861, acc: 0.5409\n",
            "Epoch 6/30\n",
            "train loss: 0.0843, acc: 0.5729\n",
            "val loss: 0.0874, acc: 0.5049\n",
            "Epoch 7/30\n",
            "train loss: 0.0860, acc: 0.5547\n",
            "val loss: 0.0883, acc: 0.4969\n",
            "Epoch 8/30\n",
            "train loss: 0.0861, acc: 0.5516\n",
            "val loss: 0.0872, acc: 0.5220\n",
            "Epoch 9/30\n",
            "train loss: 0.0856, acc: 0.5642\n",
            "val loss: 0.0888, acc: 0.5238\n",
            "Epoch 10/30\n",
            "train loss: 0.0850, acc: 0.5774\n",
            "val loss: 0.0868, acc: 0.5732\n",
            "Epoch 11/30\n",
            "train loss: 0.0865, acc: 0.5253\n",
            "val loss: 0.0877, acc: 0.5049\n",
            "Epoch 12/30\n",
            "train loss: 0.0869, acc: 0.5345\n",
            "val loss: 0.0861, acc: 0.5067\n",
            "Epoch 13/30\n",
            "train loss: 0.0851, acc: 0.5720\n",
            "val loss: 0.0869, acc: 0.5400\n",
            "Epoch 14/30\n",
            "train loss: 0.0850, acc: 0.5797\n",
            "val loss: 0.0864, acc: 0.5490\n",
            "Epoch 15/30\n",
            "train loss: 0.0842, acc: 0.5947\n",
            "val loss: 0.0857, acc: 0.5687\n",
            "Epoch 16/30\n",
            "train loss: 0.0835, acc: 0.5918\n",
            "val loss: 0.0887, acc: 0.5391\n",
            "Epoch 17/30\n",
            "train loss: 0.0831, acc: 0.5988\n",
            "val loss: 0.0853, acc: 0.5588\n",
            "Epoch 18/30\n",
            "train loss: 0.0837, acc: 0.5932\n",
            "val loss: 0.0878, acc: 0.5391\n",
            "Epoch 19/30\n",
            "train loss: 0.0820, acc: 0.6138\n",
            "val loss: 0.0861, acc: 0.5346\n",
            "Epoch 20/30\n",
            "train loss: 0.0811, acc: 0.6192\n",
            "val loss: 0.0884, acc: 0.5553\n",
            "Epoch 21/30\n",
            "train loss: 0.0813, acc: 0.6188\n",
            "val loss: 0.0904, acc: 0.5606\n",
            "Epoch 22/30\n",
            "train loss: 0.0812, acc: 0.6280\n",
            "val loss: 0.0842, acc: 0.5939\n",
            "Epoch 23/30\n",
            "train loss: 0.0798, acc: 0.6356\n",
            "val loss: 0.0878, acc: 0.5732\n",
            "Epoch 24/30\n",
            "train loss: 0.0795, acc: 0.6460\n",
            "val loss: 0.0871, acc: 0.5265\n",
            "Epoch 25/30\n",
            "train loss: 0.0793, acc: 0.6478\n",
            "val loss: 0.0880, acc: 0.5993\n",
            "Epoch 26/30\n",
            "train loss: 0.0790, acc: 0.6352\n",
            "val loss: 0.0896, acc: 0.5588\n",
            "Epoch 27/30\n",
            "train loss: 0.0773, acc: 0.6586\n",
            "val loss: 0.0853, acc: 0.6217\n",
            "Epoch 28/30\n",
            "train loss: 0.0770, acc: 0.6644\n",
            "val loss: 0.0903, acc: 0.5777\n",
            "Epoch 29/30\n",
            "train loss: 0.0763, acc: 0.6655\n",
            "val loss: 0.0915, acc: 0.5930\n",
            "Epoch 30/30\n",
            "train loss: 0.0754, acc: 0.6736\n",
            "val loss: 0.0926, acc: 0.6047\n",
            "Training complete in 1m 57s\n",
            "Best val Acc: 0.621743\n",
            "Best Epoch : 27\n",
            "Test Accuracy : 0.6518518518518519\n",
            "::: saving subject 10 ::: \n",
            "     Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1     0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2     0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "3     0.086747  0.086076   0.534292  0.597381  0.250608    21\n",
            "4     0.058172  0.072711   0.784522  0.720352  0.694018     9\n",
            "5     0.054566  0.055684   0.805017  0.800395  0.819118     2\n",
            "6     0.081580  0.074281   0.611340  0.698442  0.714465    23\n",
            "7     0.015023  0.073269   0.951613  0.886850  0.862661    28\n",
            "8     0.019791  0.155218   0.941134  0.640000  0.787234    12\n",
            "9     0.065477  0.078710   0.747047  0.686942  0.673913    12\n",
            "10    0.077331  0.085335   0.658575  0.621743  0.651852    27\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0867, acc: 0.5334\n",
            "val loss: 0.0876, acc: 0.5004\n",
            "Epoch 2/30\n",
            "train loss: 0.0867, acc: 0.5354\n",
            "val loss: 0.0868, acc: 0.5085\n",
            "Epoch 3/30\n",
            "train loss: 0.0873, acc: 0.5139\n",
            "val loss: 0.0870, acc: 0.4996\n",
            "Epoch 4/30\n",
            "train loss: 0.0868, acc: 0.5144\n",
            "val loss: 0.0871, acc: 0.5094\n",
            "Epoch 5/30\n",
            "train loss: 0.0868, acc: 0.5287\n",
            "val loss: 0.0856, acc: 0.5552\n",
            "Epoch 6/30\n",
            "train loss: 0.0860, acc: 0.5529\n",
            "val loss: 0.0887, acc: 0.5076\n",
            "Epoch 7/30\n",
            "train loss: 0.0865, acc: 0.5431\n",
            "val loss: 0.0870, acc: 0.5076\n",
            "Epoch 8/30\n",
            "train loss: 0.0865, acc: 0.5328\n",
            "val loss: 0.0872, acc: 0.5274\n",
            "Epoch 9/30\n",
            "train loss: 0.0866, acc: 0.5377\n",
            "val loss: 0.0871, acc: 0.5327\n",
            "Epoch 10/30\n",
            "train loss: 0.0867, acc: 0.5379\n",
            "val loss: 0.0871, acc: 0.5076\n",
            "Epoch 11/30\n",
            "train loss: 0.0869, acc: 0.5105\n",
            "val loss: 0.0870, acc: 0.5229\n",
            "Epoch 12/30\n",
            "train loss: 0.0870, acc: 0.5070\n",
            "val loss: 0.0873, acc: 0.5085\n",
            "Epoch 13/30\n",
            "train loss: 0.0871, acc: 0.5087\n",
            "val loss: 0.0869, acc: 0.5202\n",
            "Epoch 14/30\n",
            "train loss: 0.0872, acc: 0.5092\n",
            "val loss: 0.0887, acc: 0.4996\n",
            "Epoch 15/30\n",
            "train loss: 0.0876, acc: 0.5036\n",
            "val loss: 0.0874, acc: 0.5004\n",
            "Epoch 16/30\n",
            "train loss: 0.0871, acc: 0.5076\n",
            "val loss: 0.0872, acc: 0.5058\n",
            "Epoch 17/30\n",
            "train loss: 0.0872, acc: 0.5157\n",
            "val loss: 0.0891, acc: 0.5040\n",
            "Epoch 18/30\n",
            "train loss: 0.0872, acc: 0.5101\n",
            "val loss: 0.0885, acc: 0.4996\n",
            "Epoch 19/30\n",
            "train loss: 0.0875, acc: 0.5047\n",
            "val loss: 0.0871, acc: 0.4996\n",
            "Epoch 20/30\n",
            "train loss: 0.0870, acc: 0.5096\n",
            "val loss: 0.0884, acc: 0.5013\n",
            "Epoch 21/30\n",
            "train loss: 0.0872, acc: 0.5020\n",
            "val loss: 0.0870, acc: 0.5004\n",
            "Epoch 22/30\n",
            "train loss: 0.0870, acc: 0.5141\n",
            "val loss: 0.0866, acc: 0.5247\n",
            "Epoch 23/30\n",
            "train loss: 0.0871, acc: 0.4962\n",
            "val loss: 0.0895, acc: 0.4978\n",
            "Epoch 24/30\n",
            "train loss: 0.0872, acc: 0.5193\n",
            "val loss: 0.0869, acc: 0.5453\n",
            "Epoch 25/30\n",
            "train loss: 0.0871, acc: 0.5004\n",
            "val loss: 0.0869, acc: 0.5121\n",
            "Epoch 26/30\n",
            "train loss: 0.0873, acc: 0.5047\n",
            "val loss: 0.0870, acc: 0.5085\n",
            "Epoch 27/30\n",
            "train loss: 0.0872, acc: 0.5096\n",
            "val loss: 0.0885, acc: 0.5004\n",
            "Epoch 28/30\n",
            "train loss: 0.0870, acc: 0.5179\n",
            "val loss: 0.0888, acc: 0.4996\n",
            "Epoch 29/30\n",
            "train loss: 0.0871, acc: 0.5177\n",
            "val loss: 0.0875, acc: 0.5067\n",
            "Epoch 30/30\n",
            "train loss: 0.0867, acc: 0.5274\n",
            "val loss: 0.0872, acc: 0.5004\n",
            "Training complete in 1m 57s\n",
            "Best val Acc: 0.555157\n",
            "Best Epoch : 5\n",
            "Test Accuracy : 0.5283251231527094\n",
            "::: saving subject 11 ::: \n",
            "     Train_Loss  Val_Loss  Train_Acc   Val_Acc  Test_Acc Epoch\n",
            "1     0.086358  0.085460   0.547337  0.568047  0.299875     3\n",
            "2     0.048652  0.091221   0.831497  0.675280  0.653061    14\n",
            "3     0.086747  0.086076   0.534292  0.597381  0.250608    21\n",
            "4     0.058172  0.072711   0.784522  0.720352  0.694018     9\n",
            "5     0.054566  0.055684   0.805017  0.800395  0.819118     2\n",
            "6     0.081580  0.074281   0.611340  0.698442  0.714465    23\n",
            "7     0.015023  0.073269   0.951613  0.886850  0.862661    28\n",
            "8     0.019791  0.155218   0.941134  0.640000  0.787234    12\n",
            "9     0.065477  0.078710   0.747047  0.686942  0.673913    12\n",
            "10    0.077331  0.085335   0.658575  0.621743  0.651852    27\n",
            "11    0.086796  0.085620   0.528712  0.555157  0.528325     5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}